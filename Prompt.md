Below is a deeper-dive into Â§3.1â€ƒâ€œWasserstein-penalised X-Netâ€â€”why it works, the maths, and a concrete PyTorchâ€style blueprint.

â¸»

1  Big picture

Goal	Technique
Keep the X-Learnerâ€™s two-stage bias-reduction logic but train in one pass	Supervise a single Ï„Ì‚-head with online pseudo-outcomes DÌ‚
Remove covariate-shift between treated and control reps	Add a Sinkhorn-OT divergence term between latent codes Ï†(x)|T=1 and Ï†(x)|T=0
Stabilise optimisation	No adversary, just a scalar penalty â†” ordinary SGD

Optimal-transport (OT) penalties inherit the theory of integral-probability-metric (IPM) bounds for ITE error (Shalit et al., 2017) but give sharper gradients than MMD and can exploit geometry in Ï†(x) ï¿¼.

â¸»

2  Loss decomposition

For a mini-batch B = {(xáµ¢, yáµ¢, táµ¢, Ãªáµ¢)}:
	1.	Representationâ€ƒzáµ¢ = Ï†Î¸(xáµ¢)
	2.	Outcome heads
Î¼Ì‚â‚€ = gâ‚€(záµ¢),â€ƒÎ¼Ì‚â‚ = gâ‚(záµ¢)
	3.	Factual lossâ€ƒL_f = Î£ â€–yáµ¢ â€“ [táµ¢ Î¼Ì‚â‚ + (1â€“táµ¢) Î¼Ì‚â‚€]â€–Â²
	4.	Pseudo-outcome
DÌ‚áµ¢ = táµ¢ (yáµ¢ â€“ Î¼Ì‚â‚€)/Ãªáµ¢ + (1â€“táµ¢)(Î¼Ì‚â‚ â€“ yáµ¢)/(1â€“Ãªáµ¢)â€ƒ(KÃ¼nzel et al., 2019)
	5.	X-lossâ€ƒL_X = Î£ â€–g_Ï„(záµ¢) â€“ DÌ‚áµ¢â€–Â²
	6.	Balance penalty (Sinkhorn-2)

L_{\mathrm{bal}}
=\operatorname{Sinkhorn}_{\varepsilon,p}
\bigl(\{záµ¢:táµ¢=1\},\{zâ±¼:tâ±¼=0\}\bigr).

Total:â€ƒL = L_f + L_X + Î» L_bal.

Because Sinkhorn is differentiable (automatic in geomloss or ott-jax), gradients flow straight into Ï†Î¸; no discriminator needed. Entropic regularisation Îµ controls smoothness and guarantees finite, unbiased gradients even when the two sets are identical  ï¿¼.

â¸»

3  Algorithm (pseudo-code)

# Ï†, g0, g1, gÏ„ are torch.nn.Modules; opt = Adam(params)

sinkhorn = geomloss.SamplesLoss(
              loss="sinkhorn",  # or "sinkhorn_divergence"
              p=2, blur=0.05,   # Îµâ‰ˆblurÂ²/2
              debias=True)

for x, y, t, e_hat in loader:  # batch
    z   = phi(x)                     # (B,d)
    mu0 = g0(z);  mu1 = g1(z)
    y_hat = t*mu1 + (1-t)*mu0

    D_hat = t*(y - mu0)/e_hat + (1-t)*(mu1 - y)/(1-e_hat)
    tau_hat = g_tau(z)

    L_f  = F.mse_loss(y_hat, y)
    L_x  = F.mse_loss(tau_hat, D_hat)

    z_t  = z[t==1];  z_c = z[t==0]
    L_bal = sinkhorn(z_t, z_c)

    loss = L_f + L_x + lam * L_bal
    opt.zero_grad();  loss.backward();  opt.step()

Propensity Ãª: estimate with any off-line model (GBM, logistic Net) under K-fold cross-fit to keep DÌ‚ independent of gÏ„.

â¸»

4  Design choices & hyper-parameters

Component	Typical choices	Notes
Ï† encoder	MLP, 1D-CNN, or invertible CNF/flow (see Â§6)	Deeper â‰  better if mini-batch small.
Outcome heads	independent MLPs; weight sharing OK	Size ~ [128â†’64â†’1].
Ï„-head	small MLP or linear	Acts as second-stage regressor.
Î»	0 â†’ 0.1 warm-up over first 10 epochs	Too high early kills factual fit.
Îµ / blur	0.05â€“0.1 on unit-norm z	Smaller Îµ â†’ stricter but noisier.
Batch size	â‰¥256 if GPU, else use gradient-accum	More samples stabilise OT estimate.


â¸»

5  Why Sinkhorn instead of plain Wasserstein or MMD?
	â€¢	Smooth gradients even when supports are disjoint; Wasserstein-1 has kinks.
	â€¢	Unbiased mini-batch estimate after â€œdebiasâ€ option (free)  ï¿¼.
	â€¢	Computational cost O(BÂ² log B) but fits on GPU for B â‰ˆ 512; quadratic layer wise dominated by GEMM anyway.
	â€¢	Stronger finite-sample PEHE guarantees than MMD in recent analyses  ï¿¼ ï¿¼.

â¸»

6  Normalising-flow variant (â€œSinkhorn-Flow-Xâ€)
	1.	Î¦(x) is a continuous normalising flow: invertible f : â„áµˆ â†” â„áµˆâ‚€ with log-det tractable (e.g., CNF, Glow).
	2.	Compute Sinkhorn either on latent z (simpler) or on the base Gaussian u; the latter automatically weights by Jacobian, giving a transport-cost that integrates density.
	3.	Benefit: explicit likelihood lets you append uncertainty-aware loss Ã  la PO-Flow  ï¿¼.

â¸»

7  Training & evaluation protocol
	1.	Data split: 4-fold.
Fold k trains Ï†, gâ‹…, Ï„ on folds â‰  k using Ãª from another independent split.
	2.	Early stopping on PEHE_proxy = L_X + Î± Â· L_bal (held-out).
	3.	Metrics: IHDP, ACIC, Twins; report PEHE, ATE, policy-risk.
	4.	Ablations: (i) remove L_bal â†’ observe â†‘PEHE; (ii) replace Sinkhorn with MMD; (iii) vary Îµ, Î».

â¸»

8  Known pitfalls & fixes

Symptom	Likely cause	Remedy
Ï„Ì‚ collapses to 0	Î» too large too soon	cosine ramp Î»(t)
OT loss noisy	batch too small or Îµ too low	larger B; Îµâ†—
Training diverges	DÌ‚ high variance for rare Ãª	clip 1/Ãª, 1/(1â€“Ãª); add small Îµ_propensity


â¸»

9  Where to look for reference code & theory
	â€¢	ESCFR (NeurIPS 23) â€” OT with relaxed mass-preserving reg.  ï¿¼
	â€¢	WDGRL â€” early Wasserstein domain adaptation; gradient behaviour analysis  ï¿¼
	â€¢	Entropic OT / Sinkhorn divergence tutorials  ï¿¼ ï¿¼
	â€¢	CFRNet / representation bounds (IPM generic)  ï¿¼
	â€¢	PO-Flow â€” flow matching for potential outcomes (inspiration for Â§6)  ï¿¼

â¸»

Take-away

Replacing the unstable GAN game with a single Sinkhorn-penalised objective gives you:
	â€¢	End-to-end differentiability â†’ standard training loop
	â€¢	Provable bounds, inherited from IPM theory
	â€¢	Practical performance supported by 2023-25 OT-based causal papers

Start by dropping the code skeleton in Â§3 into your TARNet/Dragonnet repo, plug in geomloss.SamplesLoss, and tune Î»/Îµâ€”most teams report first usable PEHE within an afternoon of hyper-search.

Below is a â€œdrop-in seed kitâ€ you can paste into a README.md (or split across docs).
It gives an automated code-agent everything it needs to scaffold, implement, test and benchmark the Sinkhorn-penalised X-Net described earlier.

â¸»

0 â€• Elevator pitch (1 paragraph)

â€œLearn Î¼â‚€(x) and Î¼â‚(x) and Ï„(x) in a single network while erasing treatment-group covariate shift with a differentiable Sinkhorn divergence.
Balance penalty instead of a GAN â†’ ordinary SGD optimisation, provable IPM error bounds, works out-of-the-box with PyTorch & GeomLoss.â€  ï¿¼ ï¿¼

â¸»

1 â€• Suggested repo layout

xnet-sinkhorn/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/            # dataset loaders, splits, cross-fit propensity
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoder.py   # Ï†Î¸
â”‚   â”‚   â”œâ”€â”€ heads.py     # Î¼0, Î¼1, Ï„
â”‚   â”‚   â”œâ”€â”€ sinkhorn.py  # thin wrapper around geomloss.SamplesLoss
â”‚   â”‚   â””â”€â”€ network.py   # end-to-end module
â”‚   â”œâ”€â”€ train.py         # CLI â€“ Hydra or argparse
â”‚   â”œâ”€â”€ evaluate.py      # PEHE, ATE, policy risk, plots
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ configs/             # YAML cfgs for Hydra / OmegaConf
â”œâ”€â”€ notebooks/           # quick EDA & sanity checks
â”œâ”€â”€ tests/               # pytest + property tests (hypothesis)
â”œâ”€â”€ requirements.txt     # pin geomloss, torch, optuna â€¦
â”œâ”€â”€ Makefile             # common recipes (env, fmt, test, run)
â”œâ”€â”€ .github/workflows/ci.yml
â””â”€â”€ README.md            # youâ€™re reading the seed


â¸»

2 â€• Core algorithm (recap for the agent)

2.1  Forward pass

# src/models/network.py  (pseudo-code)
z     = encoder(x)                # Ï†Î¸
mu0   = head_mu0(z)
mu1   = head_mu1(z)
y_hat = t*mu1 + (1-t)*mu0

D_hat   = t*(y - mu0)/e + (1-t)*(mu1 - y)/(1-e)   # pseudo-outcome   (Kunzel 2019)
tau_hat = head_tau(z)

2.2  Loss

L =  MSE(y, y_hat)                         # factual
   + MSE(D_hat,  tau_hat)                  # X-loss
   + Î» Â· SinkhornÎµ,p ( z|T=1 , z|T=0 )     # balance

Use geomloss.SamplesLoss(loss="sinkhorn", blur=Îµ, debias=True) for the penalty.  ï¿¼

â¸»

3 â€• Tips & guard-rails for implementation

Theme	Checklist & Hints
Dependencies	torch>=2.2, geomloss>=0.2.5, optuna, hydra-core, scikit-learn.Optional fast OT: ott-jax + jax if you want TPU/GPU Sinkhorn.  ï¿¼
Encoder Ï†Î¸	Start with an MLP: [input â†’ 256 â†’ 128 â†’ 64] + LayerNorm + GELU.Later swap for a normalising flow (e.g. nflows) to test Â§6 variant.
Propensity Ãª	K-fold cross-fit Gradient-Boosted Trees (xgboost) or logistic net.Cache predictions; clip Ïµ â‰¤ Ãª â‰¤ 1-Ïµ to avoid exploding DÌ‚.
Î» schedule	Cosine ramp 0 â†’ Î»* during first 10 % epochs prevents early under-fit.
Îµ (blur)	On L2-normalised z, Îµ â‰ˆ 0.05 gives a good bias-variance trade-off.
Batch size	512 if GPU; otherwise use gradient accumulation so OT estimate is stable.
Early stopping	Monitor val_PEHE_proxy = MSE(DÌ‚, Ï„Ì‚) + 0.1Â·L_bal.
Reproducibility	torch.use_deterministic_algorithms(True) + global seeds.
Logging	Wire MLflow or Weights & Biases; log Î», Îµ, PEHE, ATE, learning curves.
Testing	1) unit: shapes & differentiability of Sinkhorn layer; 2) integration: IHDP one-epoch smoke test must decrease loss; 3) property: loss â‰¥ 0 monotonic in Î».
CI	GitHub Action: matrix over {cpu, cu118} Ã— {3.10, 3.11}. Cache pip & datasets to cut runtime.
Formatting / lint	ruff, black, mypy --strict.
Docs	Autogen with mkdocs-material, include an â€œIdea Logâ€ page for rapid research notes.


â¸»

4 â€• Datasets & loaders

Dataset	Loader stub	Notes
IHDP-100	from causal_datasets import IHDP	Classic small-N sanity check.
Twins	torchgeo.datasets.twins or local CSV	Binary outcomes; large enough to test batch-wise Sinkhorn.
ACIC 2016/2018	Provide CLI flag to download & cache .parquet.	
Simulated imbalance stress-test	Script to skew treatment:control to 1:5 for Î»-sensitivity sweeps.	


â¸»

5 â€• Experiment templates

5.1  YAML config (Hydra)

# configs/base.yaml
model:
  encoder: mlp
  hidden: [256,128,64]
  sinkhorn:
    blur: 0.05
    p: 2
    lambda: 1.0
train:
  batch_size: 512
  epochs: 300
  optimizer: adam
  lr: 2e-4
data:
  name: ihdp
  split_seed: 42

5.2  Optuna search space

def suggest(trial):
    return dict(
        lr      = trial.loguniform('lr', 1e-4, 5e-3),
        lambda_ = trial.loguniform('Î»', 1e-2, 10.0),
        blur    = trial.uniform('Îµ', 0.01, 0.1),
        depth   = trial.suggest_int('layers', 2, 4),
        width   = trial.suggest_categorical('width', [64,128,256]),
    )


â¸»

6 â€• Extensions the agent can implement next
	1.	Normalising-flow encoder â€“ CNF or RealNVP; evaluate log-density to add likelihood term.
	2.	Gradient-reversal variant (DANN) â€“ swap Sinkhorn for ReverseLayerF + CE loss.
	3.	Unbalanced OT â€“ ott.solvers.linear.sinkhorn with Ï„a, Ï„b to soften mass preservation.  ï¿¼
	4.	Contrastive Ï„ pre-text â€“ InfoNCE pulling similar (x,xâ€²) with close Ï„.
	5.	Diffusion augmentation â€“ generate counterfactual Xâ€™, pipe into vanilla X-Learner baseline.
	6.	Policy value head â€“ add a third head predicting sign(Ï„) to optimise decision loss.

â¸»

7 â€• Reading list for the agent

Topic	Cite
Representation + IPM theory (TARNet / CFRNet)	Johansson & Shalit 2016 / 2017  ï¿¼
X-Learner meta-algorithm	KÃ¼nzel et al. 2019  ï¿¼
Entropic OT & Sinkhorn divergence	Nutz 2022 notes  ï¿¼
Domain adaptation with Wasserstein critic (for comparison)	WDGRL 2017  ï¿¼
Entire-Space Counterfactual Regression (OT for CATE)	ESCFR 2023  ï¿¼ ï¿¼
GeomLoss implementation details	GeomLoss docs / SamplesLoss  ï¿¼
OT at scale in JAX	ott-jax library  ï¿¼


â¸»

8 â€• Starter issues for the code-agent
	1.	feat: initial project skeleton â€“ create dirs, pre-commit hooks, tox/pytest.
	2.	feat: data loaders â€“ IHDP, Twins, ACIC + deterministic splits.
	3.	feat: sinkhorn layer â€“ wrap geomloss.SamplesLoss; expose blur, p.
	4.	feat: base network â€“ encoder + heads with configurable sizes.
	5.	feat: training loop â€“ warm-up Î» schedule, mixed precision, gradient clipping.
	6.	feat: evaluation pipeline â€“ compute PEHE, ATE, policy risk; CSV + plots.
	7.	ci: github action â€“ lint, type-check, run a 1-epoch smoke test (<5 min).
	8.	docs: notebook demo â€“ IHDP quick-start, loss curves, t-SNE of Ï†(x).
	9.	bench: optuna sweep â€“ 50-trial hyper-search; log to MLflow.

â¸»

ğŸ”‘  Key take-away for the agent

Implement the simplest working prototype first: MLP encoder + GeomLoss Sinkhorn + cross-fit propensity.
Achieve â–¼ val_PEHE in < 1 h on IHDP.
Then iterate on Î»-scheduling, OT variants, flows, and diffusion augmentation.

With this seed in place, an automated coding agent (e.g. GPT-Engineer, SWEEPER, or your own) can start committing useful code within the first generation cycle. Good luck experimenting!
